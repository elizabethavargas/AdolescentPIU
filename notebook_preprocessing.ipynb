{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Actigraphy and Tabular Datasets and merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels Shape: (3960, 82)\n",
      "Memory usage reduced from 4.50 MB to 3.42 MB\n",
      "Number of features in training data: 82\n",
      "Memory usage reduced from 1.64 MB to 1.34 MB\n",
      "Number of unique training IDs: 3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Actigraphy Features: 100%|██████████| 3960/3960 [00:47<00:00, 83.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actigraphy Training Features Shape: (996, 27)\n",
      "Memory usage reduced from 0.19 MB to 0.16 MB\n",
      "Unique IDs in train_tabular: 3960\n",
      "Unique IDs in train_acti: 996\n",
      "Duplicated IDs in train_tabular: 0\n",
      "Duplicated IDs in train_acti: 0\n",
      "Merged Training Data Shape: (996, 108)\n",
      "Merged Training Data Shape: (996, 108)\n",
      "train_tabular shape: (3960, 82)\n",
      "train_acti shape: (996, 27)\n",
      "Merged DataFrame shape: (996, 108)\n",
      "Columns in train_merged:\n",
      "Index(['id', 'Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
      "       'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
      "       'Physical-Height', 'Physical-Weight',\n",
      "       ...\n",
      "       'battery_voltage_mean', 'battery_voltage_std', 'time_of_day_mean',\n",
      "       'time_of_day_std', 'weekday_mean', 'weekday_std', 'quarter_mean',\n",
      "       'quarter_std', 'relative_date_PCIAT_mean', 'relative_date_PCIAT_std'],\n",
      "      dtype='object', length=108)\n",
      "non-wear_flag_mean\n",
      "0.000000    348\n",
      "0.055029      2\n",
      "0.680572      1\n",
      "0.984497      1\n",
      "0.269607      1\n",
      "           ... \n",
      "0.814919      1\n",
      "0.028544      1\n",
      "0.537736      1\n",
      "0.058177      1\n",
      "0.592331      1\n",
      "Name: count, Length: 648, dtype: int64\n",
      "After filtering non-wear periods, shape: (348, 108)\n",
      "Original numeric_cols shape: (3960, 70)\n",
      "Imputed numeric_cols shape: (3960, 70)\n",
      "Original categorical_cols shape: (3960, 11)\n",
      "Imputed categorical_cols shape: (3960, 11)\n",
      "Memory usage reduced from 0.17 MB to 0.17 MB\n",
      "Memory usage reduced from 0.17 MB to 0.17 MB\n",
      "Processed training data saved to processed_train_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Import Necessary Libraries\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    cohen_kappa_score, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load and Preprocess Training Data\n",
    "# ----------------------------\n",
    "\n",
    "# Function to reduce memory usage\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in numerics and 'id' not in col:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(df[col].dtype).startswith('int'):\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB')\n",
    "    return df\n",
    "\n",
    "# Load training labels and tabular data\n",
    "train_csv_path = './dataset/train.csv'\n",
    "train_labels_df = pd.read_csv(train_csv_path)\n",
    "print(\"Training Labels Shape:\", train_labels_df.shape)\n",
    "\n",
    "# Reduce memory usage\n",
    "train_labels_df = reduce_memory_usage(train_labels_df)\n",
    "\n",
    "# Extract feature names directly from train_labels_df\n",
    "all_features = train_labels_df.columns.tolist()\n",
    "print(f\"Number of features in training data: {len(all_features)}\")\n",
    "\n",
    "# Add 'id' and 'sii' to the feature list if not present\n",
    "if 'id' not in all_features:\n",
    "    all_features.insert(0, 'id')\n",
    "if 'sii' not in all_features:\n",
    "    all_features.append('sii')\n",
    "\n",
    "# Select available features\n",
    "train_tabular = train_labels_df[all_features].copy()\n",
    "\n",
    "# Identify numeric and categorical columns based on data types in train_labels_df\n",
    "numeric_cols = train_tabular.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_tabular.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['id', 'sii']]\n",
    "\n",
    "# Convert 'id' to string if not already\n",
    "train_tabular['id'] = train_tabular['id'].astype(str)\n",
    "\n",
    "# Handle missing values in tabular data\n",
    "\n",
    "# Impute numerical columns with median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "train_tabular[numeric_cols] = num_imputer.fit_transform(train_tabular[numeric_cols])\n",
    "\n",
    "# Impute categorical columns with most frequent\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_tabular[categorical_cols] = cat_imputer.fit_transform(train_tabular[categorical_cols])\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in train_tabular.columns:\n",
    "        le = LabelEncoder()\n",
    "        train_tabular[col] = le.fit_transform(train_tabular[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Reduce memory usage\n",
    "train_tabular = reduce_memory_usage(train_tabular)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Extract Actigraphy Features and Merge with Tabular Data\n",
    "# ----------------------------\n",
    "\n",
    "missing_actigraphy_ids = []\n",
    "\n",
    "def extract_actigraphy_features(base_path, ids):\n",
    "    \"\"\"\n",
    "    Extract actigraphy features from partitioned Parquet files for given IDs.\n",
    "    \"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    feature_list = []\n",
    "    total_ids = len(ids)\n",
    "    for id_ in tqdm(ids, desc=\"Extracting Actigraphy Features\"):\n",
    "        id_str = str(id_)\n",
    "        # Construct the path to the id's parquet files\n",
    "        id_path = os.path.join(base_path, f'id={id_str}')\n",
    "        if not os.path.exists(id_path):\n",
    "            # print(f\"Path does not exist for id {id_str}: {id_path}\")\n",
    "            missing_actigraphy_ids.append(id_str)\n",
    "            continue\n",
    "        # Read all parquet files under this id directory\n",
    "        try:\n",
    "            acti_df = pd.read_parquet(id_path, engine='pyarrow')\n",
    "            # Add id column if not present\n",
    "            if 'id' not in acti_df.columns:\n",
    "                acti_df['id'] = id_str\n",
    "            else:\n",
    "                acti_df['id'] = acti_df['id'].astype(str)\n",
    "            # Compute features\n",
    "            feature_dict = {'id': id_str}\n",
    "            # Add all numeric features\n",
    "            numeric_columns = acti_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            for col in numeric_columns:\n",
    "                feature_dict[f'{col}_mean'] = acti_df[col].mean()\n",
    "                feature_dict[f'{col}_std'] = acti_df[col].std()\n",
    "            feature_list.append(feature_dict)\n",
    "            del acti_df\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading data for id {id_str}: {e}\")\n",
    "            missing_actigraphy_ids.append(id_str)\n",
    "            continue\n",
    "    return pd.DataFrame(feature_list)\n",
    "\n",
    "# Paths to actigraphy data\n",
    "actigraphy_train_parquet = './dataset/series_train.parquet/'\n",
    "\n",
    "# Extract unique IDs from train labels\n",
    "train_ids = train_tabular['id'].unique().tolist()\n",
    "print(f\"Number of unique training IDs: {len(train_ids)}\")\n",
    "\n",
    "# Extract actigraphy features for training set\n",
    "train_acti = extract_actigraphy_features(actigraphy_train_parquet, train_ids)\n",
    "print(\"Actigraphy Training Features Shape:\", train_acti.shape)\n",
    "\n",
    "# Reduce memory usage\n",
    "train_acti = reduce_memory_usage(train_acti)\n",
    "\n",
    "print(f\"Unique IDs in train_tabular: {train_tabular['id'].nunique()}\")\n",
    "print(f\"Unique IDs in train_acti: {train_acti['id'].nunique()}\")\n",
    "print(f\"Duplicated IDs in train_tabular: {train_tabular['id'].duplicated().sum()}\")\n",
    "print(f\"Duplicated IDs in train_acti: {train_acti['id'].duplicated().sum()}\")\n",
    "\n",
    "# Merge actigraphy features with tabular data\n",
    "train_merged = pd.merge(train_tabular, train_acti, on='id', how='inner')\n",
    "print(train_merged.columns)\n",
    "\n",
    "# apply preporcessing steps to the merged datased on ACTIGRAPHY data\n",
    "print(train_merged['non-wear_flag_mean'].value_counts())\n",
    "# Filter out non-wear periods\n",
    "if 'non-wear_flag_mean' in train_merged.columns:\n",
    "    train_merged = train_merged[train_merged['non-wear_flag_mean'] == 0]\n",
    "    print(f\"After filtering non-wear periods, shape: {train_merged.shape}\")\n",
    "\n",
    "# Remove rows where `enmo_mean` is zero\n",
    "if 'enmo_mean_mean' in train_merged.columns:\n",
    "    train_merged = train_merged[train_merged['enmo_mean_mean'] > 0]\n",
    "    print(f\"After removing rows with enmo_mean_mean == 0, shape: {train_merged.shape}\")\n",
    "\n",
    "# Replace zeros in `light_mean_mean` with NaN and impute with the column mean\n",
    "if 'light_mean_mean' in train_merged.columns:\n",
    "    train_merged['light_mean_mean'] = train_merged['light_mean_mean'].replace(0, np.nan)\n",
    "    train_merged['light_mean_mean'].fillna(train_merged['light_mean_mean'].mean(), inplace=True)\n",
    "\n",
    "# Apply log transformation to `light_mean_mean`\n",
    "if 'light_mean_mean' in train_merged.columns:\n",
    "    train_merged['log_light_mean_mean'] = np.log1p(train_merged['light_mean_mean'])  # log(1 + x) transformation\n",
    "\n",
    "merged_num_cols = train_merged.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'sii' in merged_num_cols:\n",
    "    merged_num_cols.remove('sii')\n",
    "merged_cat_cols = [col for col in train_merged.columns if col not in merged_num_cols + ['id', 'sii']]\n",
    "\n",
    "numeric_imputed = num_imputer.fit_transform(train_tabular[numeric_cols])\n",
    "\n",
    "categorical_imputed = cat_imputer.fit_transform(train_tabular[categorical_cols])\n",
    "\n",
    "# Impute numerical columns with median\n",
    "merged_imputer = SimpleImputer(strategy='median')\n",
    "train_merged[merged_num_cols] = merged_imputer.fit_transform(train_merged[merged_num_cols])\n",
    "\n",
    "# Reduce memory usage\n",
    "train_merged = reduce_memory_usage(train_merged)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Save Processed Data to CSV\n",
    "# ----------------------------\n",
    "\n",
    "# Save processed training data\n",
    "processed_train_csv_path = 'processed_train.csv'\n",
    "train_merged.to_csv(processed_train_csv_path, index=False)\n",
    "print(f\"Processed training data saved to {processed_train_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Columns: Index(['id', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score',\n",
      "       'Physical-BMI', 'Physical-Height', 'Physical-Weight',\n",
      "       'Physical-Waist_Circumference', 'Physical-Diastolic_BP',\n",
      "       'Physical-HeartRate', 'Physical-Systolic_BP',\n",
      "       'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time', 'FGC-FGC_CU',\n",
      "       'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD',\n",
      "       'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL',\n",
      "       'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL',\n",
      "       'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC',\n",
      "       'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
      "       'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
      "       'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
      "       'BIA-BIA_TBW', 'PAQ_Total', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n",
      "       'PreInt_EduHx-computerinternet_hoursday', 'sii', 'PCIAT-PCIAT_Total'],\n",
      "      dtype='object')\n",
      "Actigraphy Columns: Index(['id', 'step_mean', 'step_std', 'X_mean', 'X_std', 'Y_mean', 'Y_std',\n",
      "       'Z_mean', 'Z_std', 'enmo_mean', 'enmo_std', 'anglez_mean', 'anglez_std',\n",
      "       'non-wear_flag_mean', 'non-wear_flag_std', 'light_mean', 'light_std',\n",
      "       'battery_voltage_mean', 'battery_voltage_std', 'time_of_day_mean',\n",
      "       'time_of_day_std', 'weekday_mean', 'weekday_std', 'quarter_mean',\n",
      "       'quarter_std', 'relative_date_PCIAT_mean', 'relative_date_PCIAT_std'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "tabular = pd.read_csv('./dataset/train.csv')\n",
    "tabular.head()\n",
    "\n",
    "\"\"\"## Fixing Errors\n",
    "\n",
    "### CGAS\n",
    "The Children's Global Assessment Scale (CGAS) is a rating scale that measures the function of a child. Scores are between 1 and 100 with 1 being the most impaired.\n",
    "\"\"\"\n",
    "# find rows outside of range and fill with nan\n",
    "tabular.loc[(tabular['CGAS-CGAS_Score'] > 100) | (tabular['CGAS-CGAS_Score'] < 0),\n",
    "          'CGAS-CGAS_Score'] = float('nan')\n",
    "\n",
    "\"\"\"### Physical Features\"\"\"\n",
    "# set unrealistic weights and BMIs to NAN\n",
    "tabular.loc[tabular['Physical-BMI'] < 8, 'Physical-BMI'] = float('nan')\n",
    "\n",
    "# set unrealistic weights to NAN\n",
    "tabular.loc[tabular['Physical-Weight'] < 25, 'Physical-Weight'] = float('nan')\n",
    "\n",
    "\"\"\"### Fitness\n",
    "These are fitness stats from the FitnessGram test.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"#### Fitness_Endurance-Max_Stage\n",
    "There are only 22 stages in the endurance test so there is at least two errors. Rows with max stages below 0 or above 22 will be set to NA.\n",
    "\"\"\"\n",
    "\n",
    "# find rows outside of range and fill with nan\n",
    "tabular.loc[(tabular['Fitness_Endurance-Max_Stage'] > 22) | (tabular['Fitness_Endurance-Max_Stage'] < 0),\n",
    "          'Fitness_Endurance-Max_Stage'] = float('nan')\n",
    "\n",
    "\"\"\"#### Fitness_Endurance-Time\n",
    "Fitness_Endurance-Time is split into 2 columns with the minutes and seconds. I will combine both of these into one column.\n",
    "\"\"\"\n",
    "\n",
    "# fill na seconds with 0\n",
    "tabular['Fitness_Endurance-Time_Sec'] =  tabular['Fitness_Endurance-Time_Sec'].fillna(0)\n",
    "\n",
    "# combine minutes and seconds\n",
    "tabular['Fitness_Endurance-Time'] = tabular['Fitness_Endurance-Time_Mins'] + tabular['Fitness_Endurance-Time_Sec'] / 60\n",
    "\n",
    "# drop min and sec columns\n",
    "tabular.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'], inplace=True)\n",
    "\n",
    "\"\"\"### FitnessGram Child (FGC)\"\"\"\n",
    "\n",
    "\"\"\"#### FGC Curlups\n",
    "You can only get up to 75 curl ups so we can correct those over 75.\n",
    "\"\"\"\n",
    "\n",
    "# set all curl ups above 75 to 75 and below 0 to nan\n",
    "tabular.loc[tabular['FGC-FGC_CU'] > 75, 'FGC-FGC_CU'] = 75\n",
    "tabular.loc[tabular['FGC-FGC_CU'] < 0, 'FGC-FGC_CU'] = float('nan')\n",
    "\n",
    "# fixing trunk lift and sit and reach values\n",
    "tabular.loc[tabular['FGC-FGC_TL'] > 12, 'FGC-FGC_TL'] = 12\n",
    "tabular.loc[tabular['FGC-FGC_TL'] < 0, 'FGC-FGC_TL'] = float('nan')\n",
    "\n",
    "tabular.loc[tabular['FGC-FGC_SRR'] > 12, 'FGC-FGC_SRR'] = 12\n",
    "tabular.loc[tabular['FGC-FGC_SRR'] < 0, 'FGC-FGC_SRR'] = float('nan')\n",
    "\n",
    "tabular.loc[tabular['FGC-FGC_SRL'] > 12, 'FGC-FGC_SRL'] = 12\n",
    "tabular.loc[tabular['FGC-FGC_SRL'] < 0, 'FGC-FGC_SRL'] = float('nan')\n",
    "\n",
    "\"\"\"### BIA\n",
    "Bio-electric Impedance Analysis\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"#### Bone Mineral Content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# set extremely unrealistic BMCs to nan tp fix plot\n",
    "tabular.loc[(tabular['BIA-BIA_BMC'] > 20) | (tabular['BIA-BIA_BMC'] < 0), 'BIA-BIA_BMC'] = float('nan')\n",
    "\n",
    "\n",
    "\"\"\"#### BIA-BIA_BMI\n",
    "I already did some basic checks for Physical-BMI so I will merge in any BIA-BIA_BMIs into that column where it is missing.\n",
    "\"\"\"\n",
    "\n",
    "# fillna of Physical BMI with BIA-BIA_BMI\n",
    "tabular['Physical-BMI'] = tabular['Physical-BMI'].fillna(tabular['BIA-BIA_BMI'])\n",
    "\n",
    "# drop BIA-BIA_BMI\n",
    "tabular.drop(columns='BIA-BIA_BMI', inplace=True)\n",
    "\n",
    "\"\"\"#### Basal Metabolic Rate\"\"\"\n",
    "\n",
    "# set extremely unrealistic BMRs to nan to fix plot\n",
    "tabular.loc[(tabular['BIA-BIA_BMR'] > 5000) | (tabular['BIA-BIA_BMR'] < 500), 'BIA-BIA_BMR'] = float('nan')\n",
    "\n",
    "\"\"\"#### Daily Energy Expenditure\"\"\"\n",
    "\n",
    "# set extremely unrealistic DEEs to nan to fix plot\n",
    "tabular.loc[(tabular['BIA-BIA_DEE'] > 6000) | (tabular['BIA-BIA_DEE'] < 700), 'BIA-BIA_DEE'] = float('nan')\n",
    "\n",
    "\"\"\"#### Extracellular, Intracellular, and Total Body Water\"\"\"\n",
    "\n",
    "# set extremely unrealistic ECW to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_ECW'] > 150, 'BIA-BIA_ECW'] = float('nan')\n",
    "\n",
    "# set extremely unrealistic ICW to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_ICW'] > 150, 'BIA-BIA_ICW'] = float('nan')\n",
    "\n",
    "# set extremely unrealistic TBW to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_TBW'] > 300, 'BIA-BIA_TBW'] = float('nan')\n",
    "\n",
    "\n",
    "\"\"\"#### Fat and Fat Free Mass\"\"\"\n",
    "# set extremely unrealistic FFM to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_FFM'] > 300, 'BIA-BIA_FFM'] = float('nan')\n",
    "\n",
    "tabular[tabular['BIA-BIA_FFM'] > .98*tabular['Physical-Weight']][['Physical-Weight', 'BIA-BIA_FFM']]\n",
    "\n",
    "# set extremely unrealistic FFMI to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_FFMI'] > 30, 'BIA-BIA_FFMI'] = float('nan')\n",
    "\n",
    "# serious errors\n",
    "tabular.loc[tabular['BIA-BIA_FMI'] < 0, 'BIA-BIA_FMI']\n",
    "\n",
    "# set extremely unrealistic FMI to nan to fix plot\n",
    "tabular.loc[(tabular['BIA-BIA_FMI'] > 50) | (tabular['BIA-BIA_FMI'] < 0), 'BIA-BIA_FMI'] = float('nan')\n",
    "\n",
    "# set extremely unrealistic FMI to nan to fix plot\n",
    "tabular.loc[(tabular['BIA-BIA_Fat'] > 100) | (tabular['BIA-BIA_Fat'] < 0), 'BIA-BIA_Fat'] = float('nan')\n",
    "\n",
    "# set extremely unrealistic LDM to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_LDM'] > 100, 'BIA-BIA_LDM'] = float('nan')\n",
    "\n",
    "# set extremely unrealistic LST to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_LST'] > 300, 'BIA-BIA_LST'] = float('nan')\n",
    "\n",
    "\n",
    "# set extremely unrealistic SMM to nan to fix plot\n",
    "tabular.loc[tabular['BIA-BIA_SMM'] > 150, 'BIA-BIA_SMM'] = float('nan')\n",
    "\n",
    "\"\"\"### Physical Activity Questionnaire\n",
    "I will combine the two questionnaires into one PAQ_Total column.\n",
    "\"\"\"\n",
    "# combining reports\n",
    "tabular['PAQ_Total'] = tabular['PAQ_A-PAQ_A_Total'].fillna(tabular['PAQ_C-PAQ_C_Total'])\n",
    "\n",
    "tabular.drop(columns=['PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total'], inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"## Save Dataset & Add Actigraphy Data\n",
    "Save partially cleaned dataset for use in other models\n",
    "\"\"\"\n",
    "\n",
    "# filter columns\n",
    "tabular = tabular[['id',  'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "       'CGAS-CGAS_Score', 'Physical-BMI', 'Physical-Height', 'Physical-Weight',\n",
    "       'Physical-Waist_Circumference','Physical-Diastolic_BP', 'Physical-HeartRate',\n",
    "        'Physical-Systolic_BP','Fitness_Endurance-Max_Stage','Fitness_Endurance-Time','FGC-FGC_CU',\n",
    "        'FGC-FGC_CU_Zone', 'FGC-FGC_GSND','FGC-FGC_GSND_Zone', 'FGC-FGC_GSD',\n",
    "        'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL',\n",
    "        'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR','FGC-FGC_SRR_Zone', 'FGC-FGC_TL',\n",
    "         'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC',\n",
    "       'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "       'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "       'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "       'BIA-BIA_TBW', 'PAQ_Total', 'SDS-SDS_Total_Raw','SDS-SDS_Total_T',\n",
    "       'PreInt_EduHx-computerinternet_hoursday', 'sii', 'PCIAT-PCIAT_Total']]\n",
    "# read in\n",
    "actigraphy = pd.read_csv('processed_train.csv')\n",
    "actigraphy = actigraphy[['id', 'step_mean', 'step_std', 'X_mean', 'X_std', 'Y_mean', 'Y_std', 'Z_mean',\n",
    "       'Z_std', 'enmo_mean', 'enmo_std', 'anglez_mean', 'anglez_std',\n",
    "       'non-wear_flag_mean', 'non-wear_flag_std', 'light_mean', 'light_std',\n",
    "       'battery_voltage_mean', 'battery_voltage_std', 'time_of_day_mean',\n",
    "       'time_of_day_std', 'weekday_mean', 'weekday_std', 'quarter_mean',\n",
    "       'quarter_std', 'relative_date_PCIAT_mean', 'relative_date_PCIAT_std']]\n",
    "\n",
    "\n",
    "print(\"Tabular Columns:\", tabular.columns)\n",
    "print(\"Actigraphy Columns:\", actigraphy.columns)\n",
    "\n",
    "\n",
    "# left merge tabular and actigraphy on id\n",
    "tabular = pd.merge(tabular, actigraphy, on='id', how='left')\n",
    "tabular\n",
    "\n",
    "tabular.to_csv('train_cleaned_actigraphy.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
